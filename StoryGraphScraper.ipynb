{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu9cuO0Sz1yD"
      },
      "outputs": [],
      "source": [
        "# install selenium\n",
        "!pip install selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')  # Add the ChromeDriver path to the system path for execution.\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import numpy as np\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n"
      ],
      "metadata": {
        "id": "sMFrkNOg0Jh2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inputs are the df and timeout time - how long you want to let a page load to gather information\n",
        "def sg_scraper(df,timeout=60):\n",
        "\n",
        "    master = 'app.thestorygraph.com' #site we are using\n",
        "    search_master = '/browse?search_term=' #search url\n",
        "    results = []  #we are using this to store results for each row\n",
        "\n",
        "    # Create chrome instance and configure\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_argument('user-agent=name')\n",
        "    wd = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    wd.get('https://'+master+search_master)\n",
        "\n",
        "    close_button = WebDriverWait(wd, 10).until(\n",
        "            EC.element_to_be_clickable((By.ID, \"close-cookies-popup\"))\n",
        "        )\n",
        "\n",
        "        # Click the SVG element\n",
        "    close_button.click()\n",
        "\n",
        "    #iterate through each row\n",
        "    for index, row in df.iterrows():\n",
        "        title = row['Title']\n",
        "        author = row['Authors'].replace(\", \", \" \")\n",
        "        isbn = row['ISBN/UID']\n",
        "        format = row['Format']\n",
        "        data = {}\n",
        "\n",
        "\n",
        "        #the scraper will search with isbn first, or with title + author if isbn is null\n",
        "        if pd.isna(isbn):\n",
        "            search = f\"{title.replace(' ', '%20')}%20{author.replace(' ', '%20')}\"\n",
        "        else:\n",
        "            search = isbn\n",
        "        search_url = f\"https://{master}{search_master}{search}\"\n",
        "\n",
        "\n",
        "        #get the url for the book search\n",
        "        try:\n",
        "            wd.get(search_url)\n",
        "\n",
        "            #get the link to the book\n",
        "            book_link = wd.find_element(By.XPATH, \"//h1[@class='font-bold text-xl']/a\").get_attribute('href')\n",
        "\n",
        "            #we can also get the book id from the website\n",
        "            book_id = book_link.split(\"/books/\")[-1]\n",
        "            data['book_id'] = book_id\n",
        "            #go to the new link\n",
        "            wd.get(book_link)\n",
        "\n",
        "            #MINUTES/PAGES BLOCK\n",
        "\n",
        "            try:\n",
        "                #set minutes and pages to null\n",
        "                data['minutes'] = np.nan\n",
        "                data['pages'] = np.nan\n",
        "\n",
        "                #find text with length information\n",
        "                text = wd.find_element(By.XPATH,'/html/body/div[1]/div/main/div/div[3]/div/div[2]/p').text.strip()\n",
        "\n",
        "                #if the format is audio, we will get the hours and minutes and convert into minutes\n",
        "                if format == 'audio':\n",
        "                  #get the string to only give back numbers\n",
        "                  audio_match = re.search(r'(\\d+)\\s*hours?,\\s*(\\d+)\\s*minutes?', text)\n",
        "                  hours = int(audio_match.group(1)) if audio_match else np.nan #hours\n",
        "                  minutes = int(audio_match.group(2)) if audio_match else np.nan #minutes\n",
        "                  length = hours * 60 + minutes #length convert into minutes\n",
        "                  data['minutes'] = length #set minutes\n",
        "                #if format is not an audiobook\n",
        "                else:\n",
        "                  #get number of pages\n",
        "                  pages_match = re.search(r\"(\\d+)\\s+pages\", text)\n",
        "                  pages = int(pages_match.group(1)) if pages_match else np.nan\n",
        "                  data['pages'] = pages #set\n",
        "\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "            #PUBLISHED YEAR BLOCK\n",
        "            try:\n",
        "                #find year if it is available\n",
        "                year_match = re.search(r\"first pub (\\d{4})\", text)\n",
        "                pub_year = int(year_match.group(1)) if year_match else float('nan')\n",
        "                data['pub_year'] = pub_year\n",
        "\n",
        "                #if no pub year, set to null\n",
        "            except (AttributeError, ValueError):\n",
        "                 data['pub_year'] = np.nan\n",
        "            except Exception as e:\n",
        "                data['pub_year'] = np.nan\n",
        "\n",
        "            #GENRES BLOCK\n",
        "            try:\n",
        "              #get genres (teal text)\n",
        "              genres = [genre.text for genre in wd.find_elements(By.CLASS_NAME, 'text-teal-700') if genre.text.strip()]\n",
        "              data['genres'] = genres\n",
        "            except Exception as e:\n",
        "              data['genres'] = []\n",
        "\n",
        "            #MOOD/PACING BLOCK\n",
        "            try:\n",
        "              #get moods (pink text)\n",
        "              moods = [mood.text for mood in wd.find_elements(By.CLASS_NAME, 'text-pink-500') if mood.text.strip()]\n",
        "              data['moods'] = moods\n",
        "            except Exception as e:\n",
        "              data['moods'] = []\n",
        "\n",
        "            #SERIES BLOCK\n",
        "            try:\n",
        "              #find series, and if it exists then get series name\n",
        "              wd.find_element(By.XPATH,\"/html/body/div[1]/div/main/div/div[4]/div[1]/div[2]/div[1]/h3/p[2]/a\")\n",
        "              series_name = wd.find_element(By.XPATH,\"/html/body/div[1]/div/main/div/div[3]/div/div[2]/div[1]/h3/p[1]/a[1]\").text\n",
        "              series = True\n",
        "            #if no series is found series is set to false and returns a series name of null\n",
        "            except:\n",
        "              series = False\n",
        "              series_name = np.nan\n",
        "\n",
        "            data['series'] = series\n",
        "            data['series_name'] = series_name\n",
        "\n",
        "            #click read more button to get blurb info\n",
        "            try:\n",
        "              read_more_button = WebDriverWait(wd, 1).until(EC.element_to_be_clickable((By.CLASS_NAME, \"read-more-btn\")) )\n",
        "              read_more_button.click()\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "            #BLURB BLOCK\n",
        "            try:\n",
        "              #find blurb, otherwise return as null\n",
        "              blurb = wd.find_elements(By.CLASS_NAME, \"trix-content\")[0].text\n",
        "              data['blurb'] = blurb\n",
        "            except Exception as e:\n",
        "              data['blurb'] = np.nan\n",
        "\n",
        "            #NUMBER OF REVIEWS BLOCK\n",
        "            #this can take awhile to load, adjust timeout to how long you are willing to wait for the review count to pop up\n",
        "            try:\n",
        "              reviews = WebDriverWait(wd, timeout).until(\n",
        "                  EC.presence_of_element_located((By.CLASS_NAME, \"inverse-link\"))\n",
        "              ).text\n",
        "              #get reviews\n",
        "              match = re.search(r'([\\d,]+)\\s*reviews?', reviews)\n",
        "              review_count_str = int(match.group(1).replace(',', ''))\n",
        "              data['reviews'] = review_count_str\n",
        "            except Exception as e:\n",
        "              data['reviews'] = np.nan\n",
        "\n",
        "            #AVERAGE STAR RATING BLOCK\n",
        "            try:\n",
        "              #get star rating and convert to float\n",
        "              star_rating = float(wd.find_element(By.CLASS_NAME,\"average-star-rating\").text)\n",
        "              data['star_rating'] = star_rating\n",
        "            except Exception as e:\n",
        "              data['star_rating'] = np.nan\n",
        "\n",
        "\n",
        "\n",
        "        except (requests.exceptions.RequestException, AttributeError, TypeError, KeyError) as e:\n",
        "            print(f\"Error processing {search_url}: {e}\")\n",
        "            #all the new columns and old\n",
        "            data['book_id'], data['pages'], data['minutes'], data['pub_year'], data['genres'], data['moods'], data['series'], data['series_name'], data['blurb'], data['reviews'], data['star_rating'] = np.nan,np.nan, np.nan,np.nan, [], [], np.nan, np.nan, np.nan, np.nan, np.nan\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred(general): {e}\")\n",
        "            data['book_id'], data['pages'], data['minutes'], data['pub_year'], data['genres'], data['moods'], data['series'], data['series_name'], data['blurb'], data['reviews'], data['star_rating'] = np.nan,np.nan, np.nan,np.nan, [], [], np.nan, np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "        results.append(data)\n",
        "\n",
        "    # Create new columns from the results\n",
        "    df['book_id'] = [result.get('book_id', np.nan) for result in results]\n",
        "    df['pub_year'] = [result.get('pub_year', np.nan) for result in results]\n",
        "    df['genres'] = [result.get('genres', []) for result in results]\n",
        "    df['moods'] = [result.get('moods', []) for result in results]\n",
        "    df['series'] = [result.get('series', False) for result in results]\n",
        "    df['series_name'] = [result.get('series_name', np.nan) for result in results]\n",
        "    df['blurb'] = [result.get('blurb', np.nan) for result in results]\n",
        "    df['reviews'] = [result.get('reviews', np.nan) for result in results]\n",
        "    df['star_rating'] = [result.get('star_rating', np.nan) for result in results]\n",
        "    df['minutes'] = [result.get('minutes', np.nan) for result in results]\n",
        "    df['pages'] = [result.get('pages', np.nan) for result in results]\n",
        "\n",
        "\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "3tbJxI4_0cRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload your dataframe to whatever the name and location is\n",
        "df = pd.read_csv('storygraphExport.csv')"
      ],
      "metadata": {
        "id": "reUPadY81fld"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change the timeout to however long you are willing to wait to let a page load for each book, in seconds\n",
        "scraped = sg_scraper(df,timeout=60)"
      ],
      "metadata": {
        "id": "GWZIsQCD5h6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraped.to_csv('scraped_storygraphExport.csv') #change this to whatever name you like"
      ],
      "metadata": {
        "id": "yk_d4T2VBbI6"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}